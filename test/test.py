import torch

def check_cuda() -> None:
    print("ðŸ” VÃ©rification de la disponibilitÃ© CUDA...\n")

    # VÃ©rification de base
    cuda_available = torch.cuda.is_available()
    mps_available = torch.backends.mps.is_available()

    if cuda_available:
        device_name = torch.cuda.get_device_name(0)
        device_count = torch.cuda.device_count()
        total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        print(f"âœ… CUDA disponible")
        print(f"ðŸ§  GPU dÃ©tectÃ© : {device_name}")
        print(f"ðŸ”¢ Nombre de GPU : {device_count}")
        print(f"ðŸ’¾ MÃ©moire totale : {total_mem:.2f} Go")
        print(f"ðŸ“¦ Version PyTorch CUDA : {torch.version.cuda}")
        print(f"âš™ï¸  Version cudNN : {torch.backends.cudnn.version()}")
    elif mps_available:
        print("âš™ï¸  CUDA indisponible, mais MPS (Apple Silicon) dÃ©tectÃ©.")
        print("âœ… EntraÃ®nement possible sur GPU Apple.")
    else:
        print("âŒ Aucun GPU dÃ©tectÃ©.")
        print("ðŸ§© ExÃ©cution sur CPU uniquement.")

    print("\nðŸ”§ Device utilisÃ© par dÃ©faut :", get_best_device())

def get_best_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")

if __name__ == "__main__":
    check_cuda()
